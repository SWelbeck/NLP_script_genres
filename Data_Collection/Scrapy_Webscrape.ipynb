{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:38:02.161405Z",
     "start_time": "2021-02-02T22:38:01.437197Z"
    }
   },
   "outputs": [],
   "source": [
    "import scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:38:02.503447Z",
     "start_time": "2021-02-02T22:38:02.172691Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from progress.bar import Bar\n",
    "# from plotline_utilities import progression_bar\n",
    "from selenium import webdriver\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:38:02.521595Z",
     "start_time": "2021-02-02T22:38:02.505746Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_movies():\n",
    "     # Parse the page http://www.imsdb.com/all%20scripts/ with beautiful soup\n",
    "    link_all_scripts = 'https://imsdb.com/all-scripts.html'\n",
    "    response_all_scripts = requests.get(link_all_scripts)\n",
    "    soup = BeautifulSoup(response_all_scripts.text, 'html.parser')\n",
    "\n",
    "    # This webpage is constructed with tables, the 3rd one is the one we want\n",
    "    find_tables = soup.findAll('td', valign='top')\n",
    "    all_movies = find_tables[2].findAll('a')\n",
    "\n",
    "    movies = [(movie_info.string, \\\n",
    "              movie_info[\"href\"], \\\n",
    "              re.split(\"[,.]\",movie_info.string)[0].replace(' ', '_'))\n",
    "              for movie_info in all_movies]\n",
    "    return movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:38:02.529535Z",
     "start_time": "2021-02-02T22:38:02.524446Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_movie_info(movies):\n",
    "\n",
    "    for movie in movies:\n",
    "        if movie[1][0:15] !='/Movie Scripts/':\n",
    "            return 'One of the movie link does not start with /Movie Scripts/.'\n",
    "    return 'All movie URLs have a correct format.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:38:05.907938Z",
     "start_time": "2021-02-02T22:38:05.865300Z"
    }
   },
   "outputs": [],
   "source": [
    "def handle_movie (movie, browser):\n",
    "\n",
    "    # Unpack tuple\n",
    "    title, link_to_movie_page, movie_title = movie\n",
    "\n",
    "    # Interrogate the page with all the movie information (ratings, writer,\n",
    "    # genre, link to script)\n",
    "    full_html_link = u'http://www.imsdb.com' + link_to_movie_page\n",
    "    response_script = requests.get(full_html_link)\n",
    "    soup = BeautifulSoup(response_script.text, 'html.parser')\n",
    "\n",
    "    # Get all relevant information (genre, writer, script) from page\n",
    "    list_links = soup.find_all('table', align=\"center\")[0].find_all('a')\n",
    "    print(len(list_links))\n",
    "    genre = []\n",
    "    writer = []\n",
    "    script = ''\n",
    "    for link in list_links:\n",
    "        href = link['href']\n",
    "        if href[0:7]== \"/writer\":\n",
    "            writer.append(link.get_text())\n",
    "        if href[0:7]== \"/genre/\":\n",
    "            genre.append(link.get_text())\n",
    "        if href[0:9]== \"/scripts/\":\n",
    "            script = href\n",
    "\n",
    "    # If the link to the script points to a PDF, skip this movie, but log\n",
    "    # the information in `movies_pdf_script.csv`\n",
    "    if script == '' or script[-5:] != '.html':\n",
    "        path_to_directory = '../data/scraping/'\n",
    "        pdf_logging_filename = path_to_directory + 'movies_pdf_script.csv'\n",
    "        with open(pdf_logging_filename, 'a') as f:\n",
    "            new_row = title + '\\n'\n",
    "            f.write(new_row)\n",
    "\n",
    "    # If the link to the script points to an html page, write the corresponding\n",
    "    # text to a file and include the movie in a csv file, with meta-information\n",
    "    else:\n",
    "\n",
    "        # Parse the webpage which contains the script text\n",
    "        full_script_url =  u'http://www.imsdb.com' + script\n",
    "        browser.get(full_script_url)\n",
    "        page_text = browser.page_source\n",
    "        soup = BeautifulSoup(page_text, 'html.parser')\n",
    "\n",
    "        # If the scraping does not go as planned (unexpected structure),\n",
    "        # log the file name in an error file\n",
    "        if len(soup.find_all('td', \"scrtext\"))!=1:\n",
    "            error_file_name = '../data/scraping/scraping_error.csv'\n",
    "            with open(error_file_name, 'a') as error_file:\n",
    "                new_row = title + '\\n'\n",
    "                error_file.write( new_row )\n",
    "\n",
    "        # Normal scraping:\n",
    "        else:\n",
    "            # Write the script text to a file\n",
    "            path_to_directory = '../data/scraping/texts/'\n",
    "            filename = path_to_directory + movie_title + '.txt'\n",
    "            text = soup.findAll('td', \"scrtext\")[0].get_text()\n",
    "            with codecs.open(filename, \"w\",\n",
    "                    encoding='ascii', errors='ignore') as f:\n",
    "                f.write(text)\n",
    "\n",
    "            # Add the meta-information to a CSV file\n",
    "            path_to_directory = '../data/scraping/'\n",
    "            success_filename = path_to_directory + 'successful_files.csv'\n",
    "            new_row = title + ';' + str(genre) + ';' + str(writer) + ';' \\\n",
    "                    + movie_title + ';' + filename + '\\n'\n",
    "            with open(success_filename, 'a') as f:\n",
    "                f.write(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T00:21:40.360090Z",
     "start_time": "2021-02-03T00:20:57.008333Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All movie URLs have a correct format.\n",
      "4\n",
      "6\n",
      "5\n",
      "5\n",
      "5\n",
      "7\n",
      "11\n",
      "7\n",
      "4\n",
      "5\n",
      "3\n",
      "8\n",
      "8\n",
      "5\n",
      "4\n",
      "7\n",
      "4\n",
      "8\n",
      "3\n",
      "5\n",
      "7\n",
      "7\n",
      "5\n",
      "6\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "\n",
    "    # Create data/scraping/texts files\n",
    "    if not os.path.exists('../data'):\n",
    "        os.mkdir('../data')\n",
    "        print ('making ../data folder')\n",
    "    if not os.path.exists('../data/scraping'):\n",
    "        os.mkdir('../data/scraping')\n",
    "        print ('making ../data/scraping folder')\n",
    "    if not os.path.exists('../data/scraping/texts'):\n",
    "        os.mkdir('../data/scraping/texts')\n",
    "        print ('making ../data/scraping/texts folder')   \n",
    "        \n",
    "    # List all the available movies, and the corresponding URL links\n",
    "    movies = get_all_movies()\n",
    "    print (check_movie_info(movies))\n",
    "\n",
    "    \n",
    "    # Write all the scripts (in texts folder) and the summary of the movies\n",
    "    # in .csv format (in scraping folder)\n",
    "    browser = webdriver.Safari()\n",
    "    for idx,movie in enumerate(movies[1179:]):\n",
    "        handle_movie(movie, browser)\n",
    "        Bar(len(movies))\n",
    "        \n",
    "#     for i,movie in enumerate(movies[822:]):\n",
    "#         handle_movie(movie, browser)\n",
    "#         Bar(len(movies))\n",
    "\n",
    "#messed up, after Nurse betty, 1167, 1178\"\"\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env1",
   "language": "python",
   "name": "new-env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
